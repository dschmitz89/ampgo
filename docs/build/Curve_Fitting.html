

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Curve Fitting &mdash; simplenlopt 1.0 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="simplenlopt" href="modules.html" />
    <link rel="prev" title="In depth: Gradients" href="InDepth_Gradients.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home"> simplenlopt
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="Quickstart.html">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="Constrained_Optimization.html">Constrained optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="Global_Opt.html">Global Optimization</a></li>
<li class="toctree-l1"><a class="reference internal" href="InDepth_Gradients.html">In depth: Gradients</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Curve Fitting</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#fitting-a-simple-model">Fitting a simple model</a></li>
<li class="toctree-l2"><a class="reference internal" href="#troubleshooting-difficult-fits">Troubleshooting difficult fits</a></li>
<li class="toctree-l2"><a class="reference internal" href="#robust-fits-for-outlier-handling">Robust fits for outlier handling</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="modules.html">simplenlopt</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">simplenlopt</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Curve Fitting</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="_sources/Curve_Fitting.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="section" id="curve-fitting">
<h1>Curve Fitting<a class="headerlink" href="#curve-fitting" title="Permalink to this headline">¶</a></h1>
<p>NLopt does not contain dedicated algorithms for curve fitting but any
nonlinear optimization algorithm can be used for curve fitting. The
curve_fit function is a simple convenience function to make curve
fitting more straightforward. As for general local and global
optimization, the API mimics SciPy to make testing different algorithms
as simple as possible. Unlike SciPy, all available local optimizers can
be employed to perform the fitting.</p>
<div class="section" id="fitting-a-simple-model">
<h2>Fitting a simple model<a class="headerlink" href="#fitting-a-simple-model" title="Permalink to this headline">¶</a></h2>
<p>In this tutorial we will recreate SciPy’s curve fitting
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html">example</a>.</p>
<p>The model to be fitted has to be supplied as
<code class="docutils literal notranslate"><span class="pre">f(x,</span> <span class="pre">param_1,</span> <span class="pre">param_2,</span> <span class="pre">...</span> <span class="pre">param_n)</span></code> where <code class="docutils literal notranslate"><span class="pre">x</span></code> denotes the
independent variable and the other variables the model parameters we
want to estimate. The three-parameter model to be fitted is given by an
exponential decay plus an offset:</p>
<div class="math notranslate nohighlight">
\[\begin{equation} f(x|a, b, c)=c + a\cdot e^{-b\cdot x} \end{equation}\]</div>
<p>Least squares fitting means that the sum of the squared differences
between the predictions made by the model and the observed data is
minimized. In this case:</p>
<div class="math notranslate nohighlight">
\[\begin{equation} \underset{a, b, c}{\mathrm{argmin}}\sum_i \bigl(f(x_i|a, b, c)-y_i\bigl)^2 \end{equation}\]</div>
<p>First, we set ground truth parameters, generate a model prediction and
add some gaussian noise.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">simplenlopt</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
</pre></div>
</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>

    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span>

<span class="n">xdata</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span><span class="mi">50</span><span class="p">)</span>
<span class="n">a_true</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">b_true</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">c_true</span> <span class="o">=</span> <span class="mf">2.</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">a_true</span><span class="p">,</span> <span class="n">b_true</span><span class="p">,</span> <span class="n">c_true</span><span class="p">)</span>
<span class="n">y_data</span> <span class="o">=</span> <span class="n">y</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">xdata</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="Curve_Fitting_files%5CCurve_Fitting_3_0.png" src="Curve_Fitting_files%5CCurve_Fitting_3_0.png" />
<p>Now we can fit the model using the curve_fit function. Its calling
signature is the same as SciPy’s: its inputs are at least a Python
function which calculates the model prediction and both the independent
variable <code class="docutils literal notranslate"><span class="pre">xdata</span></code> and the observed data <code class="docutils literal notranslate"><span class="pre">ydata</span></code>. In case of
successsfull optimization it returns the estimated model parameters and
the model parameter covariance matrix. The square root of the covariance
matrix’s diagonal yields an estimate of the model parameter errors.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params</span><span class="p">,</span> <span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">simplenlopt</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> <span class="n">y_data</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Estimated model parameters: &quot;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>

<span class="n">std_errors</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">cov_matrix</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model parameter standard errors: &quot;</span><span class="p">,</span> <span class="n">std_errors</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">params</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Estimated</span> <span class="n">model</span> <span class="n">parameters</span><span class="p">:</span>  <span class="p">[</span><span class="mf">1.98126468</span> <span class="mf">0.89576888</span> <span class="mf">2.05230148</span><span class="p">]</span>
<span class="n">Model</span> <span class="n">parameter</span> <span class="n">standard</span> <span class="n">errors</span><span class="p">:</span>  <span class="p">[</span><span class="mf">0.00478004</span> <span class="mf">0.00594988</span> <span class="mf">0.00357008</span><span class="p">]</span>
</pre></div>
</div>
<img alt="Curve_Fitting_files%5CCurve_Fitting_5_1.png" src="Curve_Fitting_files%5CCurve_Fitting_5_1.png" />
<p>Unlike scipy, simplenlopt will use a gradient-free algorithm (BOBYQA) if
no gradient information is supplied. Gradient information must be
supplied as an N x m numpy array for N data points and m parameters.
Each column of the gradient matrix must contain the partial derivative
with respect to one parameter for all data points. To make this less
confusing, the code below shows how to supply the gradient for this
example. The partial derivatives are</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align}
\frac{\partial f}{\partial a} &amp;=e^{-b\cdot x} \\
\frac{\partial f}{\partial b} &amp;=-x\cdot a\cdot e^{-b\cdot x} \\
\frac{\partial f}{\partial c} &amp;=1
\end{align}\end{split}\]</div>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">jac_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">):</span>

    <span class="n">da</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">db</span> <span class="o">=</span> <span class="o">-</span><span class="n">x</span> <span class="o">*</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">dc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full_like</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">1.</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">stack</span><span class="p">((</span><span class="n">da</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">db</span><span class="o">.</span><span class="n">ravel</span><span class="p">(),</span> <span class="n">dc</span><span class="p">),</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>

<span class="n">params</span><span class="p">,</span> <span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">simplenlopt</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> <span class="n">y_data</span><span class="p">,</span> <span class="n">jac</span> <span class="o">=</span> <span class="n">jac_model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Estimated model parameters: &quot;</span><span class="p">,</span> <span class="n">params</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Estimated</span> <span class="n">model</span> <span class="n">parameters</span><span class="p">:</span>  <span class="p">[</span><span class="mf">1.98126516</span> <span class="mf">0.89577841</span> <span class="mf">2.05230726</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="troubleshooting-difficult-fits">
<h2>Troubleshooting difficult fits<a class="headerlink" href="#troubleshooting-difficult-fits" title="Permalink to this headline">¶</a></h2>
<p>Next, let’s try a more complicated example to show how the choice of
optimizers and starting guesses affects the fitting results. We will add
a gaussian peak to the model to make it more complex:</p>
<div class="math notranslate nohighlight">
\[\begin{equation} f(x|a, b, c, d, e, f)=c + a\cdot e^{-b\cdot x} +d\cdot \exp\left(\frac{-(e-x)^2}{f^2}\right) \end{equation}\]</div>
<p>Again, let’s first generate some fake data.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">complex_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">e</span><span class="p">,</span> <span class="n">f</span><span class="p">):</span>

    <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">e</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">f</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>

<span class="n">d_true</span> <span class="o">=</span> <span class="mf">1.5</span>
<span class="n">e_true</span> <span class="o">=</span> <span class="mf">3.</span>
<span class="n">f_true</span> <span class="o">=</span> <span class="mf">0.5</span>

<span class="n">true_params</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">a_true</span><span class="p">,</span> <span class="n">b_true</span><span class="p">,</span> <span class="n">c_true</span><span class="p">,</span> <span class="n">d_true</span><span class="p">,</span> <span class="n">e_true</span><span class="p">,</span> <span class="n">f_true</span><span class="p">])</span>
<span class="n">y_complex</span> <span class="o">=</span> <span class="n">complex_model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">true_params</span><span class="p">)</span>
<span class="n">y_noisy</span> <span class="o">=</span> <span class="n">y_complex</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">xdata</span><span class="o">.</span><span class="n">size</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">matplotlib</span><span class="o">.</span><span class="n">collections</span><span class="o">.</span><span class="n">PathCollection</span> <span class="n">at</span> <span class="mh">0x1f4d841b370</span><span class="o">&gt;</span>
</pre></div>
</div>
<img alt="Curve_Fitting_files%5CCurve_Fitting_9_1.png" src="Curve_Fitting_files%5CCurve_Fitting_9_1.png" />
<p>By default, the initial guess for all model parameters is 1. This guess
is far away from the true parameters in this example. A starting point
far away from the true parameters also makes it hard for the optimizer
to find the true parameters. To illustrate this, let’s plot the model
prediction for the initial guesses and the parameters found by the
default optimizer, BOBYQA.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_guesses</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">optimized_params</span><span class="p">,</span> <span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">simplenlopt</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">complex_model</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">complex_model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">initial_guesses</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Initial guess&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">complex_model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">optimized_params</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimized&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">7</span><span class="o">-</span><span class="mf">57e43</span><span class="n">c5c3f19</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">3</span><span class="p">:</span> <span class="ne">RuntimeWarning</span><span class="p">:</span> <span class="n">divide</span> <span class="n">by</span> <span class="n">zero</span> <span class="n">encountered</span> <span class="ow">in</span> <span class="n">true_divide</span>
  <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">e</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">f</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<img alt="Curve_Fitting_files%5CCurve_Fitting_11_1.png" src="Curve_Fitting_files%5CCurve_Fitting_11_1.png" />
<p>Ugh, obviously a bad fit. Let’s try different starting guesses: we guess
the peak position to be at 2.5.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">initial_guesses</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">,</span> <span class="mf">1.</span><span class="p">])</span>
<span class="n">optimized_params</span><span class="p">,</span> <span class="n">cov_matrix</span> <span class="o">=</span> <span class="n">simplenlopt</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">complex_model</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">p0</span> <span class="o">=</span> <span class="n">initial_guesses</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">complex_model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">initial_guesses</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Initial guess&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">complex_model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">optimized_params</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimized&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">&lt;</span><span class="n">ipython</span><span class="o">-</span><span class="nb">input</span><span class="o">-</span><span class="mi">7</span><span class="o">-</span><span class="mf">57e43</span><span class="n">c5c3f19</span><span class="o">&gt;</span><span class="p">:</span><span class="mi">3</span><span class="p">:</span> <span class="ne">RuntimeWarning</span><span class="p">:</span> <span class="n">divide</span> <span class="n">by</span> <span class="n">zero</span> <span class="n">encountered</span> <span class="ow">in</span> <span class="n">true_divide</span>
  <span class="k">return</span> <span class="n">a</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">b</span> <span class="o">*</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span> <span class="o">+</span> <span class="n">d</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">e</span><span class="o">-</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="o">/</span><span class="n">f</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<img alt="Curve_Fitting_files%5CCurve_Fitting_13_1.png" src="Curve_Fitting_files%5CCurve_Fitting_13_1.png" />
<p>This looks much better! Keep in mind that it is always better to supply
an initial estimate for the model parameters. If your parameters are far
away from 1, fitting will not work without it.</p>
<p>That being said, it is also always a good idea to test different
optimizers. We will try to fit the model using the default guesses of 1
for all parameters using the Method of Moving Asymptotes (MMA) and
compare it with the BOBYQA result.</p>
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">params_bobyqa</span><span class="p">,</span> <span class="n">bobyqa_cov</span> <span class="o">=</span> <span class="n">simplenlopt</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">complex_model</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;BOBYQA&#39;</span><span class="p">)</span>
<span class="n">params_mma</span><span class="p">,</span> <span class="n">mma_cov</span> <span class="o">=</span> <span class="n">simplenlopt</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span><span class="n">complex_model</span><span class="p">,</span> <span class="n">xdata</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">method</span> <span class="o">=</span> <span class="s1">&#39;MMA&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">y_noisy</span><span class="p">,</span> <span class="n">c</span> <span class="o">=</span> <span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">complex_model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">params_bobyqa</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;BOBYQA&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="n">complex_model</span><span class="p">(</span><span class="n">xdata</span><span class="p">,</span> <span class="o">*</span><span class="n">params_mma</span><span class="p">),</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;MMA&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<pre class="literal-block">&lt;ipython-input-7-57e43c5c3f19&gt;:3: RuntimeWarning: divide by zero encountered in true_divide
  return a * np.exp(-b * x) + c + d * np.exp(-(e-x)**2/f**2)
C:Usersdanie.condaenvssimplenlopt_envlibsite-packagessimplenlopt_Core.py:178: RuntimeWarning: Using gradient-based optimization, but no gradient information is available. Gradient will be approximated by central difference. Consider using a derivative-free optimizer or supplying gradient information.
  warn('Using gradient-based optimization'</pre>
<img alt="Curve_Fitting_files%5CCurve_Fitting_15_1.png" src="Curve_Fitting_files%5CCurve_Fitting_15_1.png" />
<p>Clearly, MMA outperforms BOBYQA in this case. As always with nonlinear
optimization though, this does not have to be true for all models.</p>
</div>
<div class="section" id="robust-fits-for-outlier-handling">
<h2>Robust fits for outlier handling<a class="headerlink" href="#robust-fits-for-outlier-handling" title="Permalink to this headline">¶</a></h2>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="modules.html" class="btn btn-neutral float-right" title="simplenlopt" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="InDepth_Gradients.html" class="btn btn-neutral float-left" title="In depth: Gradients" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2021, Daniel Schmitz.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>